{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawtober\n",
    "## LSTM for a list of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = \"prompts_formatted.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert chars to ints\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  23952\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.9696\n",
      "Epoch 00001: loss improved from inf to 2.96964, saving model to weights-improvement-01-2.9696-bigger.hdf5\n",
      "375/375 [==============================] - 295s 788ms/step - loss: 2.9696\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.4301\n",
      "Epoch 00002: loss improved from 2.96964 to 2.43009, saving model to weights-improvement-02-2.4301-bigger.hdf5\n",
      "375/375 [==============================] - 299s 797ms/step - loss: 2.4301\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.2636\n",
      "Epoch 00003: loss improved from 2.43009 to 2.26363, saving model to weights-improvement-03-2.2636-bigger.hdf5\n",
      "375/375 [==============================] - 335s 894ms/step - loss: 2.2636\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.1820\n",
      "Epoch 00004: loss improved from 2.26363 to 2.18197, saving model to weights-improvement-04-2.1820-bigger.hdf5\n",
      "375/375 [==============================] - 380s 1s/step - loss: 2.1820\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.1264\n",
      "Epoch 00005: loss improved from 2.18197 to 2.12645, saving model to weights-improvement-05-2.1264-bigger.hdf5\n",
      "375/375 [==============================] - 338s 902ms/step - loss: 2.1264\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.0452\n",
      "Epoch 00006: loss improved from 2.12645 to 2.04520, saving model to weights-improvement-06-2.0452-bigger.hdf5\n",
      "375/375 [==============================] - 288s 769ms/step - loss: 2.0452\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.9913\n",
      "Epoch 00007: loss improved from 2.04520 to 1.99133, saving model to weights-improvement-07-1.9913-bigger.hdf5\n",
      "375/375 [==============================] - 267s 711ms/step - loss: 1.9913\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.9376\n",
      "Epoch 00008: loss improved from 1.99133 to 1.93764, saving model to weights-improvement-08-1.9376-bigger.hdf5\n",
      "375/375 [==============================] - 271s 723ms/step - loss: 1.9376\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.9055\n",
      "Epoch 00009: loss improved from 1.93764 to 1.90553, saving model to weights-improvement-09-1.9055-bigger.hdf5\n",
      "375/375 [==============================] - 275s 732ms/step - loss: 1.9055\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.8767\n",
      "Epoch 00010: loss improved from 1.90553 to 1.87672, saving model to weights-improvement-10-1.8767-bigger.hdf5\n",
      "375/375 [==============================] - 277s 739ms/step - loss: 1.8767\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.8484\n",
      "Epoch 00011: loss improved from 1.87672 to 1.84839, saving model to weights-improvement-11-1.8484-bigger.hdf5\n",
      "375/375 [==============================] - 272s 725ms/step - loss: 1.8484\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.8176\n",
      "Epoch 00012: loss improved from 1.84839 to 1.81756, saving model to weights-improvement-12-1.8176-bigger.hdf5\n",
      "375/375 [==============================] - 263s 701ms/step - loss: 1.8176\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.8001\n",
      "Epoch 00013: loss improved from 1.81756 to 1.80012, saving model to weights-improvement-13-1.8001-bigger.hdf5\n",
      "375/375 [==============================] - 241s 642ms/step - loss: 1.8001\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.7795\n",
      "Epoch 00014: loss improved from 1.80012 to 1.77953, saving model to weights-improvement-14-1.7795-bigger.hdf5\n",
      "375/375 [==============================] - 224s 598ms/step - loss: 1.7795\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.7427\n",
      "Epoch 00015: loss improved from 1.77953 to 1.74273, saving model to weights-improvement-15-1.7427-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.7427\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.7202\n",
      "Epoch 00016: loss improved from 1.74273 to 1.72024, saving model to weights-improvement-16-1.7202-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.7202\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.6932\n",
      "Epoch 00017: loss improved from 1.72024 to 1.69316, saving model to weights-improvement-17-1.6932-bigger.hdf5\n",
      "375/375 [==============================] - 222s 593ms/step - loss: 1.6932\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.6682\n",
      "Epoch 00018: loss improved from 1.69316 to 1.66818, saving model to weights-improvement-18-1.6682-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.6682\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.6501\n",
      "Epoch 00019: loss improved from 1.66818 to 1.65005, saving model to weights-improvement-19-1.6501-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.6501\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.6140\n",
      "Epoch 00020: loss improved from 1.65005 to 1.61399, saving model to weights-improvement-20-1.6140-bigger.hdf5\n",
      "375/375 [==============================] - 222s 593ms/step - loss: 1.6140\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.5780\n",
      "Epoch 00021: loss improved from 1.61399 to 1.57797, saving model to weights-improvement-21-1.5780-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.5780\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.5519\n",
      "Epoch 00022: loss improved from 1.57797 to 1.55192, saving model to weights-improvement-22-1.5519-bigger.hdf5\n",
      "375/375 [==============================] - 222s 591ms/step - loss: 1.5519\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.5242\n",
      "Epoch 00023: loss improved from 1.55192 to 1.52417, saving model to weights-improvement-23-1.5242-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.5242\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.4825\n",
      "Epoch 00024: loss improved from 1.52417 to 1.48248, saving model to weights-improvement-24-1.4825-bigger.hdf5\n",
      "375/375 [==============================] - 223s 594ms/step - loss: 1.4825\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 00025: loss improved from 1.48248 to 1.43830, saving model to weights-improvement-25-1.4383-bigger.hdf5\n",
      "375/375 [==============================] - 222s 591ms/step - loss: 1.4383\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.4126\n",
      "Epoch 00026: loss improved from 1.43830 to 1.41258, saving model to weights-improvement-26-1.4126-bigger.hdf5\n",
      "375/375 [==============================] - 221s 590ms/step - loss: 1.4126\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.3719\n",
      "Epoch 00027: loss improved from 1.41258 to 1.37194, saving model to weights-improvement-27-1.3719-bigger.hdf5\n",
      "375/375 [==============================] - 222s 591ms/step - loss: 1.3719\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.3294\n",
      "Epoch 00028: loss improved from 1.37194 to 1.32942, saving model to weights-improvement-28-1.3294-bigger.hdf5\n",
      "375/375 [==============================] - 221s 591ms/step - loss: 1.3294\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.2846\n",
      "Epoch 00029: loss improved from 1.32942 to 1.28457, saving model to weights-improvement-29-1.2846-bigger.hdf5\n",
      "375/375 [==============================] - 222s 592ms/step - loss: 1.2846\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.2443\n",
      "Epoch 00030: loss improved from 1.28457 to 1.24428, saving model to weights-improvement-30-1.2443-bigger.hdf5\n",
      "375/375 [==============================] - 553s 1s/step - loss: 1.2443\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.2248\n",
      "Epoch 00031: loss improved from 1.24428 to 1.22479, saving model to weights-improvement-31-1.2248-bigger.hdf5\n",
      "375/375 [==============================] - 1416s 4s/step - loss: 1.2248\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.1652\n",
      "Epoch 00032: loss improved from 1.22479 to 1.16520, saving model to weights-improvement-32-1.1652-bigger.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1423s 4s/step - loss: 1.1652\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.1115\n",
      "Epoch 00033: loss improved from 1.16520 to 1.11154, saving model to weights-improvement-33-1.1115-bigger.hdf5\n",
      "375/375 [==============================] - 1428s 4s/step - loss: 1.1115\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.0695\n",
      "Epoch 00034: loss improved from 1.11154 to 1.06950, saving model to weights-improvement-34-1.0695-bigger.hdf5\n",
      "375/375 [==============================] - 1455s 4s/step - loss: 1.0695\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.0376\n",
      "Epoch 00035: loss improved from 1.06950 to 1.03756, saving model to weights-improvement-35-1.0376-bigger.hdf5\n",
      "375/375 [==============================] - 1423s 4s/step - loss: 1.0376\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.9829\n",
      "Epoch 00036: loss improved from 1.03756 to 0.98285, saving model to weights-improvement-36-0.9829-bigger.hdf5\n",
      "375/375 [==============================] - 1424s 4s/step - loss: 0.9829\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.9492\n",
      "Epoch 00037: loss improved from 0.98285 to 0.94920, saving model to weights-improvement-37-0.9492-bigger.hdf5\n",
      "375/375 [==============================] - 1429s 4s/step - loss: 0.9492\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.9135\n",
      "Epoch 00038: loss improved from 0.94920 to 0.91349, saving model to weights-improvement-38-0.9135-bigger.hdf5\n",
      "375/375 [==============================] - 1421s 4s/step - loss: 0.9135\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.8693\n",
      "Epoch 00039: loss improved from 0.91349 to 0.86934, saving model to weights-improvement-39-0.8693-bigger.hdf5\n",
      "375/375 [==============================] - 1418s 4s/step - loss: 0.8693\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.8245\n",
      "Epoch 00040: loss improved from 0.86934 to 0.82455, saving model to weights-improvement-40-0.8245-bigger.hdf5\n",
      "375/375 [==============================] - 1420s 4s/step - loss: 0.8245\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.7966\n",
      "Epoch 00041: loss improved from 0.82455 to 0.79660, saving model to weights-improvement-41-0.7966-bigger.hdf5\n",
      "375/375 [==============================] - 1418s 4s/step - loss: 0.7966\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.7694\n",
      "Epoch 00042: loss improved from 0.79660 to 0.76945, saving model to weights-improvement-42-0.7694-bigger.hdf5\n",
      "375/375 [==============================] - 1444s 4s/step - loss: 0.7694\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.7100\n",
      "Epoch 00043: loss improved from 0.76945 to 0.70999, saving model to weights-improvement-43-0.7100-bigger.hdf5\n",
      "375/375 [==============================] - 1422s 4s/step - loss: 0.7100\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.7025\n",
      "Epoch 00044: loss improved from 0.70999 to 0.70248, saving model to weights-improvement-44-0.7025-bigger.hdf5\n",
      "375/375 [==============================] - 1420s 4s/step - loss: 0.7025\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.6519\n",
      "Epoch 00045: loss improved from 0.70248 to 0.65193, saving model to weights-improvement-45-0.6519-bigger.hdf5\n",
      "375/375 [==============================] - 1417s 4s/step - loss: 0.6519\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.6263\n",
      "Epoch 00046: loss improved from 0.65193 to 0.62627, saving model to weights-improvement-46-0.6263-bigger.hdf5\n",
      "375/375 [==============================] - 1417s 4s/step - loss: 0.6263\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.6083\n",
      "Epoch 00047: loss improved from 0.62627 to 0.60828, saving model to weights-improvement-47-0.6083-bigger.hdf5\n",
      "375/375 [==============================] - 1415s 4s/step - loss: 0.6083\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.5630\n",
      "Epoch 00048: loss improved from 0.60828 to 0.56297, saving model to weights-improvement-48-0.5630-bigger.hdf5\n",
      "375/375 [==============================] - 1420s 4s/step - loss: 0.5630\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.5440\n",
      "Epoch 00049: loss improved from 0.56297 to 0.54395, saving model to weights-improvement-49-0.5440-bigger.hdf5\n",
      "375/375 [==============================] - 1416s 4s/step - loss: 0.5440\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.5136\n",
      "Epoch 00050: loss improved from 0.54395 to 0.51357, saving model to weights-improvement-50-0.5136-bigger.hdf5\n",
      "375/375 [==============================] - 1422s 4s/step - loss: 0.5136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b03c1cfc40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-0.5136-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "('\"', 'nd\\n\\nartsy october 2020:\\n1.\\ttender\\n2.\\tshelter\\n3.\\timpulse\\n4.\\tcorset\\n5.\\tgarden\\n6.\\tdevotion\\n7.\\tarrow\\n8.\\t', '\"')\n"
     ]
    }
   ],
   "source": [
    "#pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "#to start at the beginning:\n",
    "#pattern= dataX[0]\n",
    "\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eless\n",
      "9.\tsae\n",
      "10.\tflow\n",
      "11.\tsweet\n",
      "12.\tloon\n",
      "13.\tsaale\n",
      "14.\twolf\n",
      "15.\tsoof\n",
      "16.\tmonn\n",
      "17.\tmot\n",
      "18.\tfale\n",
      "19.\trragedy\n",
      "20.\tclov\n",
      "21.\tmosd\n",
      "22.\thure\n",
      "23.\treary\n",
      "24.\tgiass\n",
      "25.\tlantern\n",
      "26.\tsuiptir\n",
      "27.\tgeaths\n",
      "29.\tscarec\n",
      "20.\tfards\n",
      "31.\tfieet\n",
      "22.\tcreature\n",
      "23.\tshadows\n",
      "24.\tdevild\n",
      "25.\tharden\n",
      "26.\tmesd\n",
      "27.\tcont\n",
      "28.\tgards\n",
      "29.\tsciseer\n",
      "30.\tsine\n",
      "31.\tcuemn\n",
      "22.\tbuild\n",
      "23.\trisionamion\n",
      "24.\tbanele\n",
      "25.\tdoeednt\n",
      "26.\tseale\n",
      "27.\tcurnos\n",
      "18.\tgrisgy\n",
      "20.\tlonring\n",
      "31.\tcrate\n",
      "22.\twind\n",
      "23.\trepi\n",
      "24.\tserine\n",
      "25.\tsword\n",
      "26.\tantels\n",
      "27.\tjead\n",
      "28.\tmuthond\n",
      "20.\tsears\n",
      "\n",
      "witchtober 2018:\n",
      "1.\tapprentice\n",
      "2.\ttea\n",
      "3.\tnerdy\n",
      "4.\tsea\n",
      "5.\tlunar\n",
      "6.\tjornir\n",
      "7.\teangy\n",
      "8.\tstel\n",
      "9.\tswing\n",
      "10.\tpattern\n",
      "11.\tbug\n",
      "12.\tcoffin\n",
      "13.\tpiciic\n",
      "14.\tmotiorsateo\n",
      "15.\tbremen\n",
      "16.\toosses\n",
      "17.\tlever\n",
      "18.\tswist\n",
      "30.\tfurming\n",
      "31.\teyeloa\n",
      "22.\tgorw\n",
      "23.\terow\n",
      "24.\tciock\n",
      "25.\tsea\n",
      "26.\tlues\n",
      "27.\truing\n",
      "28.\tfioonc\n",
      "29.\tboy\n",
      "30.\tdegd\n",
      "31.\toesessed\n",
      "\n",
      "inktober prompts:\n",
      "1.\ta character\n",
      "2.\tlove interest\n",
      "3.\tpets\n",
      "4.\thore\n",
      "5.\tjob\n",
      "6.\tfavorite outri\n",
      "7.\tluleet \n",
      "8.\tgiathc\n",
      "9.\tclaieleon\n",
      "10.\tbegonia\n",
      "11.\trabbit\n",
      "12.\tourfit of the day\n",
      "13.\tburning\n",
      "14.\tholeiis 15.\tood gods\n",
      "16.\tfavorite boos\n",
      "18.\tlever\n",
      "19.\tshite bedd\n",
      "30.\thile 31.\teneett \n",
      "drawtober:\n",
      "1.\thashion\n",
      "2.\teemon\n",
      "3.\tskeleton\n",
      "4.\tcogeyman\n",
      "5.\tjack-o-lantern\n",
      "6.\tgollling\n",
      "7.\tgamdee\n",
      "8.\tmeghnel 9.\talowet\n",
      "10.\tfauerell\n",
      "11.\tfug\n",
      "12.\tcoldss\n",
      "13.\tcucila\n",
      "14.\tmolsecrl\n",
      "15.\tgarr\n",
      "16.\tgrltecn\n",
      "17.\tshale\n",
      "18.\tdoltred\n",
      "19.\tsping\n",
      "20.\ttings\n",
      "21.\tpeddp\n",
      "22.\tchef\n",
      "23.\trip\n",
      "24.\tdig\n",
      "25.\tbuddy\n",
      "26.\thide\n",
      "27.\tmusic\n",
      "28.\tfloat\n",
      "29.\tshoes\n",
      "30.\tominous\n",
      "31.\tcrawl\n",
      "\n",
      "inktober 2020:\n",
      "1.\tmuimit\n",
      "2.\tdefile\n",
      "3.\tunssop\n",
      "4.\tcores\n",
      "5.\twerea\n",
      "6.\tfxes\n",
      "7.\tveap\n",
      "8.\tpatehed\n",
      "10.\troreons\n",
      "11.\tfuter\n",
      "12.\twiale\n",
      "13.\tgel\n",
      "14.\tcat\n",
      "15.\tcelosia\n",
      "16.\twoll\n",
      "17.\trealet\n",
      "18.\tlhase\n",
      "30.\tsmeep\n",
      "31.\thaune\n",
      "22.\tbreama\n",
      "23.\tresi\n",
      "24.\tsia\n",
      "25.\tsiane\n",
      "26.\taatire\n",
      "27.\telow\n",
      "28.\trrak\n",
      "29.\tdolure\n",
      "30.\tcate\n",
      "31.\tfiod\n",
      "\n",
      "artober challenge:\n",
      "1.\tbaking\n",
      "2.\ttropical\n",
      "3.\tcoffee\n",
      "4.\tflowers\n",
      "5.\taaueden\n",
      "6.\tdrogs\n",
      "7.\tcancl\n",
      "8.\tsea\n",
      "9.\tcochen\n",
      "10.\tleaies\n",
      "11.\tfoagom\n",
      "12.\taochen\n",
      "13.\tmotions\n",
      "14.\tcoone\n",
      "15.\twelp\n",
      "16.\tmong\n",
      "17.\tcite\n",
      "18.\trrage\n",
      "29.\tblotu\n",
      "20.\teiooing\n",
      "31.\tmoogtule\n",
      "\n",
      "artober challenge:\n",
      "1.\tba\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(2000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it again, unformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  15357\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"prompts_list.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-evenbigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 3.0570\n",
      "Epoch 00001: loss improved from inf to 3.05702, saving model to weights-improvement-01-3.0570-evenbigger.hdf5\n",
      "240/240 [==============================] - 132s 549ms/step - loss: 3.0570\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 3.0011\n",
      "Epoch 00002: loss improved from 3.05702 to 3.00109, saving model to weights-improvement-02-3.0011-evenbigger.hdf5\n",
      "240/240 [==============================] - 137s 571ms/step - loss: 3.0011\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.9400\n",
      "Epoch 00003: loss improved from 3.00109 to 2.93997, saving model to weights-improvement-03-2.9400-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 2.9400\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.8204\n",
      "Epoch 00004: loss improved from 2.93997 to 2.82042, saving model to weights-improvement-04-2.8204-evenbigger.hdf5\n",
      "240/240 [==============================] - 140s 583ms/step - loss: 2.8204\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7802\n",
      "Epoch 00005: loss improved from 2.82042 to 2.78024, saving model to weights-improvement-05-2.7802-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 2.7802\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7587\n",
      "Epoch 00006: loss improved from 2.78024 to 2.75867, saving model to weights-improvement-06-2.7587-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 2.7587\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7488\n",
      "Epoch 00007: loss improved from 2.75867 to 2.74879, saving model to weights-improvement-07-2.7488-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 2.7488\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7390\n",
      "Epoch 00008: loss improved from 2.74879 to 2.73905, saving model to weights-improvement-08-2.7390-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 2.7390\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7339\n",
      "Epoch 00009: loss improved from 2.73905 to 2.73391, saving model to weights-improvement-09-2.7339-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 2.7339\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7222\n",
      "Epoch 00010: loss improved from 2.73391 to 2.72218, saving model to weights-improvement-10-2.7222-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 576ms/step - loss: 2.7222\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7150\n",
      "Epoch 00011: loss improved from 2.72218 to 2.71499, saving model to weights-improvement-11-2.7150-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 577ms/step - loss: 2.7150\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.7049\n",
      "Epoch 00012: loss improved from 2.71499 to 2.70491, saving model to weights-improvement-12-2.7049-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 575ms/step - loss: 2.7049\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.6925\n",
      "Epoch 00013: loss improved from 2.70491 to 2.69250, saving model to weights-improvement-13-2.6925-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 577ms/step - loss: 2.6925\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.6789\n",
      "Epoch 00014: loss improved from 2.69250 to 2.67894, saving model to weights-improvement-14-2.6789-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 577ms/step - loss: 2.6789\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.6632\n",
      "Epoch 00015: loss improved from 2.67894 to 2.66319, saving model to weights-improvement-15-2.6632-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 2.6632\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.6440\n",
      "Epoch 00016: loss improved from 2.66319 to 2.64404, saving model to weights-improvement-16-2.6440-evenbigger.hdf5\n",
      "240/240 [==============================] - 140s 584ms/step - loss: 2.6440\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.6230\n",
      "Epoch 00017: loss improved from 2.64404 to 2.62296, saving model to weights-improvement-17-2.6230-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 575ms/step - loss: 2.6230\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.5955\n",
      "Epoch 00018: loss improved from 2.62296 to 2.59547, saving model to weights-improvement-18-2.5955-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 2.5955\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.5728\n",
      "Epoch 00019: loss improved from 2.59547 to 2.57285, saving model to weights-improvement-19-2.5728-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 576ms/step - loss: 2.5728\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.5369\n",
      "Epoch 00020: loss improved from 2.57285 to 2.53685, saving model to weights-improvement-20-2.5369-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 579ms/step - loss: 2.5369\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.4953\n",
      "Epoch 00021: loss improved from 2.53685 to 2.49534, saving model to weights-improvement-21-2.4953-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 576ms/step - loss: 2.4953\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.4617\n",
      "Epoch 00022: loss improved from 2.49534 to 2.46167, saving model to weights-improvement-22-2.4617-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 579ms/step - loss: 2.4617\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.4191\n",
      "Epoch 00023: loss improved from 2.46167 to 2.41911, saving model to weights-improvement-23-2.4191-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 579ms/step - loss: 2.4191\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.3636\n",
      "Epoch 00024: loss improved from 2.41911 to 2.36363, saving model to weights-improvement-24-2.3636-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 2.3636\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.3168\n",
      "Epoch 00025: loss improved from 2.36363 to 2.31682, saving model to weights-improvement-25-2.3168-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 2.3168\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.2618\n",
      "Epoch 00026: loss improved from 2.31682 to 2.26176, saving model to weights-improvement-26-2.2618-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 579ms/step - loss: 2.2618\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.2047\n",
      "Epoch 00027: loss improved from 2.26176 to 2.20473, saving model to weights-improvement-27-2.2047-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 581ms/step - loss: 2.2047\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.1350\n",
      "Epoch 00028: loss improved from 2.20473 to 2.13504, saving model to weights-improvement-28-2.1350-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 579ms/step - loss: 2.1350\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.0717\n",
      "Epoch 00029: loss improved from 2.13504 to 2.07169, saving model to weights-improvement-29-2.0717-evenbigger.hdf5\n",
      "240/240 [==============================] - 140s 585ms/step - loss: 2.0717\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.9968\n",
      "Epoch 00030: loss improved from 2.07169 to 1.99680, saving model to weights-improvement-30-1.9968-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 577ms/step - loss: 1.9968\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.9323\n",
      "Epoch 00031: loss improved from 1.99680 to 1.93235, saving model to weights-improvement-31-1.9323-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 1.9323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.8519\n",
      "Epoch 00032: loss improved from 1.93235 to 1.85190, saving model to weights-improvement-32-1.8519-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.8519\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.7686\n",
      "Epoch 00033: loss improved from 1.85190 to 1.76863, saving model to weights-improvement-33-1.7686-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.7686\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.7043\n",
      "Epoch 00034: loss improved from 1.76863 to 1.70426, saving model to weights-improvement-34-1.7043-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.7043\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.6259\n",
      "Epoch 00035: loss improved from 1.70426 to 1.62593, saving model to weights-improvement-35-1.6259-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.6259\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.5591\n",
      "Epoch 00036: loss improved from 1.62593 to 1.55906, saving model to weights-improvement-36-1.5591-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.5591\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.4835\n",
      "Epoch 00037: loss improved from 1.55906 to 1.48348, saving model to weights-improvement-37-1.4835-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.4835\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.3991\n",
      "Epoch 00038: loss improved from 1.48348 to 1.39914, saving model to weights-improvement-38-1.3991-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 1.3991\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.3320\n",
      "Epoch 00039: loss improved from 1.39914 to 1.33195, saving model to weights-improvement-39-1.3320-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 577ms/step - loss: 1.3320\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2697\n",
      "Epoch 00040: loss improved from 1.33195 to 1.26969, saving model to weights-improvement-40-1.2697-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.2697\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2021\n",
      "Epoch 00041: loss improved from 1.26969 to 1.20214, saving model to weights-improvement-41-1.2021-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 575ms/step - loss: 1.2021\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.1318\n",
      "Epoch 00042: loss improved from 1.20214 to 1.13180, saving model to weights-improvement-42-1.1318-evenbigger.hdf5\n",
      "240/240 [==============================] - 140s 584ms/step - loss: 1.1318\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.0671\n",
      "Epoch 00043: loss improved from 1.13180 to 1.06710, saving model to weights-improvement-43-1.0671-evenbigger.hdf5\n",
      "240/240 [==============================] - 138s 576ms/step - loss: 1.0671\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.0017\n",
      "Epoch 00044: loss improved from 1.06710 to 1.00166, saving model to weights-improvement-44-1.0017-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 1.0017\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 0.9498\n",
      "Epoch 00045: loss improved from 1.00166 to 0.94979, saving model to weights-improvement-45-0.9498-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 0.9498\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 0.8904\n",
      "Epoch 00046: loss improved from 0.94979 to 0.89036, saving model to weights-improvement-46-0.8904-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 0.8904\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 0.8443\n",
      "Epoch 00047: loss improved from 0.89036 to 0.84427, saving model to weights-improvement-47-0.8443-evenbigger.hdf5\n",
      "240/240 [==============================] - 140s 583ms/step - loss: 0.8443\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 0.8047\n",
      "Epoch 00048: loss improved from 0.84427 to 0.80468, saving model to weights-improvement-48-0.8047-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 580ms/step - loss: 0.8047\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 0.7383\n",
      "Epoch 00049: loss improved from 0.80468 to 0.73828, saving model to weights-improvement-49-0.7383-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 578ms/step - loss: 0.7383\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - ETA: 0s - loss: 0.7094\n",
      "Epoch 00050: loss improved from 0.73828 to 0.70943, saving model to weights-improvement-50-0.7094-evenbigger.hdf5\n",
      "240/240 [==============================] - 139s 581ms/step - loss: 0.7094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x190c2c57dc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model2.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-26-1.5242-bigger.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'int_to_char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4de953ff1076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Seed:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-4de953ff1076>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Seed:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'int_to_char' is not defined"
     ]
    }
   ],
   "source": [
    "#pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "#to start at the beginning:\n",
    "#pattern= dataX[0]\n",
    "\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "burning\n",
      "hollow\n",
      "old gods\n",
      "favorite book\n",
      "cet\n",
      "rwict\n",
      "rtat\n",
      "fevor\n",
      "instic\n",
      "flate\n",
      "tenpt\n",
      "cat\n",
      "rein\n",
      "clffenl\n",
      "foog\n",
      "swals\n",
      "sattad\n",
      "lhart\n",
      "cat\n",
      "bloos\n",
      "eear\n",
      "dloes\n",
      "hide\n",
      "master\n",
      "aioonation\n",
      "shadow\n",
      "bool\n",
      "lion\n",
      "sacrer\n",
      "shadow\n",
      "banl\n",
      "moss\n",
      "cres\n",
      "andhent\n",
      "flnf\n",
      "lild\n",
      "spale\n",
      "paten\n",
      "wisches\n",
      "spear\n",
      "blood\n",
      "detire\n",
      "broken\n",
      "mismarch\n",
      "saarn\n",
      "aburrite shale\n",
      "poierion\n",
      "coacknt\n",
      "foggin\n",
      "golg\n",
      "gaslrite outfit\n",
      "sider gam\n",
      "lels\n",
      "nightmare\n",
      "buinding\n",
      "holidt stopd\n",
      "heaser\n",
      "wcseh\n",
      "crrvume\n",
      "boainn\n",
      "conk\n",
      "swans\n",
      "soater\n",
      "fate\n",
      "tomf\n",
      "rream\n",
      "crielrile\n",
      "flowing\n",
      "harrest\n",
      "blond\n",
      "srork\n",
      "suap\n",
      "dizzy\n",
      "coral\n",
      "sleep\n",
      "chef\n",
      "rip\n",
      "dig\n",
      "buddy\n",
      "hide\n",
      "music\n",
      "float\n",
      "shoes\n",
      "ominous\n",
      "crawl\n",
      "shadow\n",
      "bqeem\n",
      "harvest\n",
      "faml\n",
      "mask\n",
      "mose\n",
      "anrtu\n",
      "darsid\n",
      "eat\n",
      "books\n",
      "ereaninn\n",
      "foowing\n",
      "harridtt\n",
      "bluiq\n",
      "soare\n",
      "pe the day\n",
      "black cat\n",
      "bow and arrow\n",
      "memento\n",
      "mori\n",
      "crystals\n",
      "shadow\n",
      "beloved\n",
      "snack\n",
      "therter\n",
      "raren\n",
      "eandle\n",
      "magic\n",
      "voodoo\n",
      "fere\n",
      "plant\n",
      "catirest\n",
      "brach\n",
      "macic\n",
      "villalce\n",
      "intien swatcre\n",
      "beac cat\n",
      "civisibll\n",
      "ampme\n",
      "sioch\n",
      "fmpmat\n",
      "gantist\n",
      "famoy haz\n",
      "swist\n",
      "foowe\n",
      "becak gate\n",
      "cireuedl\n",
      "garc\n",
      "mask\n",
      "cursed\n",
      "urban legend\n",
      "lost\n",
      "crescent moon\n",
      "cittane\n",
      "boowie\n",
      "forndtt\n",
      "brece\n",
      "maciid\n",
      "flll\n",
      "aeter\n",
      "toeep\n",
      "smore\n",
      "ted chttary\n",
      "wine\n",
      "knifee\n",
      "waror\n",
      "hores\n",
      "bne kobsiing\n",
      "garning\n",
      "gairy\n",
      "hid\n",
      "gall\n",
      "iaiins\n",
      "forns\n",
      "floues\n",
      "shadow\n",
      "beanvesy\n",
      "bountid\n",
      "bouse\n",
      "mask\n",
      "meat\n",
      "mois\n",
      "pocen\n",
      "fandy\n",
      "siit\n",
      "clowne\n",
      "forest\n",
      "demes\n",
      "coose\n",
      "auile\n",
      "husly\n",
      "enowing\n",
      "fangs\n",
      "tea\n",
      "clack catthetl\n",
      "alaci\n",
      "sche\n",
      "ootion\n",
      "hanmiig\n",
      "gairy\n",
      "hid\n",
      "fall\n",
      "iaiins\n",
      "oodeit\n",
      "fac\n",
      "yird\n",
      "rores\n",
      "femonia\n",
      "werch\n",
      "flooume\n",
      "seates\n",
      "cat\n",
      "poist\n",
      "femon\n",
      "seie\n",
      "soock\n",
      "soace\n",
      "toord\n",
      "gnons\n",
      "heses\n",
      "soase\n",
      "pe the day\n",
      "burning\n",
      "hollow\n",
      "old gods\n",
      "favorite book\n",
      "cat\n",
      "reaket\n",
      "mastirn\n",
      "babrie\n",
      "ooneic\n",
      "frewm\n",
      "nasrreal\n",
      "fard moise\n",
      "amdery\n",
      "antpler\n",
      "oidele ages\n",
      "lovecraftian\n",
      "nocturnal\n",
      "cold grave\n",
      "jack-o-lantern\n",
      "forest\n",
      "demon\n",
      "shadows\n",
      "haunted house\n",
      "mask\n",
      "meat\n",
      "poison\n",
      "ghost \n",
      "spider\n",
      "potions\n",
      "pumpkin\n",
      "sator\n",
      "firot\n",
      "sniser\n",
      "cathr\n",
      "mnttirg\n",
      "ganged\n",
      "folget\n",
      "gorrs\n",
      "elownie\n",
      "boackirg\n",
      "collo hyytaly\n",
      "black\n",
      "cat\n",
      "potions\n",
      "moth\n",
      "tie poak\n",
      "bngkent\n",
      "flr batele\n",
      "croken\n",
      "mask\n",
      "mirror\n",
      "enpnmmy\n",
      "fally\n",
      "blulles\n",
      "siater\n",
      "cancles\n",
      "ocgen\n",
      "lonioro\n",
      "rotion\n",
      "skice\n",
      "torrdh\n",
      "fartiva\n",
      "cooes\n",
      "erow\n",
      "midnight\n",
      "sedth\n",
      "dryil\n",
      "orin\n",
      "have\n",
      "canrhrned\n",
      "grrns\n",
      "elownse\n",
      "bloes\n",
      "breakamm\n",
      "suier cleat\n",
      "faoble\n",
      "broken\n",
      "maresti\n",
      "collo hlosing\n",
      "hang\n",
      "most\n",
      "skeee\n",
      "ieliuery\n",
      "suamp\n",
      "pushroom\n",
      "duston\n",
      "shalow\n",
      "misales\n",
      "sporeh\n",
      "distarce\n",
      "cisci\n",
      "smake\n",
      "putation\n",
      "santy\n",
      "hope\n",
      "dist\n",
      "sriasert\n",
      "desoning\n",
      "fracant\n",
      "zrrpee\n",
      "siace\n",
      "mesou\n",
      "corisal\n",
      "poselns\n",
      "oark\n",
      "cepiriar\n",
      "flrifin amimal firotion\n",
      "suici\n",
      "faro\n",
      "gage\n",
      "porion\n",
      "horns\n",
      "lounalged\n",
      "fanemes\n",
      "oumpkin\n",
      "fater\n",
      "mokear\n",
      "fittt\n",
      "aorsle\n",
      "boxes\n",
      "seales\n",
      "cuagon\n",
      "rndent\n",
      "fage\n",
      "yore\n",
      "miditmare\n",
      "cuinding\n",
      "holidt\n",
      "stopg\n",
      "eyes\n",
      "shadow\n",
      "bloom\n",
      "book\n",
      "muis\n",
      "cetlan\n",
      "rooekr\n",
      "sldne\n",
      "toace\n",
      "ceoou\n",
      "gorsing\n",
      "hang\n",
      "most\n",
      "sreet\n",
      "natter\n",
      "snack\n",
      "catbret\n",
      "gerrine\n",
      "searisiia\n",
      "elat\n",
      "cat\n",
      "rean\n",
      "cremo\n",
      "roint\n",
      "doath\n",
      "dect\n",
      "ooijing\n",
      "goig\n",
      "spant\n",
      "craath\n",
      "bberroe\n",
      "booken\n",
      "cngerari\n",
      "booket\n",
      "owmpkin\n",
      "fally\n",
      "blodl\n",
      "nisy\n",
      "trarer\n",
      "soace\n",
      "chtture\n",
      "sitmal\n",
      "water\n",
      "canniire\n",
      "ceari\n",
      "midrecice\n",
      "fal\n",
      "poikanie\n",
      "seroy hlrsing\n",
      "hllgs\n",
      "clote\n",
      "siin\n",
      "cevituation\n",
      "cene\n",
      "dosn\n",
      "stile\n",
      "trace\n",
      "sream nrtared\n",
      "rare\n",
      "mushriom\n",
      "pution\n",
      "cat\n",
      "doakk\n",
      "cse\n",
      "teene\n",
      "ootiong\n",
      "hern\n",
      "mistale\n",
      "bortume\n",
      "sebel\n",
      "haunt\n",
      "carved\n",
      "bat\n",
      "crash\n",
      "mirror\n",
      "demonit\n",
      "sid\n",
      "pordy\n",
      "geroive\n",
      "sering\n",
      "forte\n",
      "eamidt\n",
      "tiater\n",
      "paten\n",
      "uater\n",
      "water\n",
      "wetoam\n",
      "snerl\n",
      "soeas\n",
      "rronkn\n",
      "conning\n",
      "folle alokdt\n",
      "twiet\n",
      "ttatic\n",
      "cry\n",
      "door\n",
      "eege\n",
      "shards\n",
      "moon\n",
      "reflect\n",
      "sealed\n",
      "dirt\n",
      "falling\n",
      "gagge\n",
      "demot\n",
      "medengal fivotite sening\n",
      "cork\n",
      "traam\n",
      "anile\n",
      "treent\n",
      "farey\n",
      "teeth\n",
      "throw\n",
      "hope\n",
      "disgusting\n",
      "slippery\n",
      "dune\n",
      "armor\n",
      "outpong\n",
      "altte\n",
      "mioh\n",
      "mach\n",
      "micter\n",
      "seapee\n",
      "canniiar firifid\n",
      "faterite\n",
      "garneny\n",
      "beandenous\n",
      "sheep\n",
      "spics\n",
      "crefls\n",
      "bnass\n",
      "sweet\n",
      "ttat\n",
      "fheel\n",
      "so boe suree\n",
      "dronalt\n",
      "feme\n",
      "eemet\n",
      "rotion\n",
      "falmicar\n",
      "sea\n",
      "bbandoned murpe\n",
      "feood\n",
      "tpias fress\n",
      "poinan\n",
      "fattis iisische\n",
      "blnne\n",
      "bemonite\n",
      "lantirn\n",
      "jaumtid house\n",
      "mask\n",
      "meat\n",
      "moos\n",
      "pade\n",
      "fareies\n",
      "oum\n",
      "tienet\n",
      "shadow\n",
      "bano\n",
      "motn\n",
      "cres\n",
      "hodsy\n",
      "seine\n",
      "paliiile\n",
      "meiiniay\n",
      "flaient\n",
      "tleep\n",
      "stirm\n",
      "trap\n",
      "dizzy\n",
      "coral\n",
      "sleep\n",
      "chef\n",
      "rip\n",
      "dig\n",
      "buddy\n",
      "hide\n",
      "music\n",
      "float\n",
      "shoes\n",
      "ominous\n",
      "crawl\n",
      "shadow\n",
      "bqeem\n",
      "harvest\n",
      "faml\n",
      "mask\n",
      "mose\n",
      "anrtu\n",
      "darsid\n",
      "eat\n",
      "books\n",
      "ereaninn\n",
      "foowing\n",
      "harridtt\n",
      "bluiq\n",
      "soare\n",
      "pe the day\n",
      "black cat\n",
      "bow and arrow\n",
      "memento\n",
      "mori\n",
      "crystals\n",
      "shadow\n",
      "beloved\n",
      "snack\n",
      "therter\n",
      "raren\n",
      "eandle\n",
      "magic\n",
      "voodoo\n",
      "fere\n",
      "plant\n",
      "catirest\n",
      "brach\n",
      "macic\n",
      "villalce\n",
      "intien swatcre\n",
      "beac cat\n",
      "civisibll\n",
      "ampme\n",
      "sioch\n",
      "fmpmat\n",
      "gantist\n",
      "famoy haz\n",
      "swist\n",
      "foowe\n",
      "becak gate\n",
      "cireuedl\n",
      "garc\n",
      "mask\n",
      "cursed\n",
      "urban legend\n",
      "lost\n",
      "crescent moon\n",
      "cittane\n",
      "boowie\n",
      "forndtt\n",
      "brece\n",
      "maciid\n",
      "flll\n",
      "aeter\n",
      "toeep\n",
      "smore\n",
      "ted chttary\n",
      "wine\n",
      "knifee\n",
      "waror\n",
      "hores\n",
      "bne kobsiing\n",
      "garning\n",
      "gairy\n",
      "hid\n",
      "gall\n",
      "iaiins\n",
      "forns\n",
      "floues\n",
      "shadow\n",
      "beanvesy\n",
      "bountid\n",
      "bouse\n",
      "mask\n",
      "meat\n",
      "mois\n",
      "pocen\n",
      "fandy\n",
      "siit\n",
      "clowne\n",
      "forest\n",
      "demes\n",
      "coose\n",
      "auile\n",
      "husly\n",
      "enowing\n",
      "fangs\n",
      "tea\n",
      "clack catthetl\n",
      "alaci\n",
      "sche\n",
      "ootion\n",
      "hanmiig\n",
      "gairy\n",
      "hid\n",
      "fall\n",
      "iaiins\n",
      "oodeit\n",
      "fac\n",
      "yird\n",
      "rores\n",
      "femonia\n",
      "werch\n",
      "flooume\n",
      "seates\n",
      "cat\n",
      "poist\n",
      "femon\n",
      "seie\n",
      "soock\n",
      "soace\n",
      "toord\n",
      "gnons\n",
      "heses\n",
      "soase\n",
      "pe the day\n",
      "burning\n",
      "hollow\n",
      "old gods\n",
      "favorite book\n",
      "cat\n",
      "reaket\n",
      "mastirn\n",
      "babrie\n",
      "ooneic\n",
      "frewm\n",
      "nasrreal\n",
      "fard moise\n",
      "amdery\n",
      "antpler\n",
      "oidele ages\n",
      "lovecraftian\n",
      "nocturnal\n",
      "cold grave\n",
      "jack-o-lantern\n",
      "forest\n",
      "demon\n",
      "shadows\n",
      "haunted house\n",
      "mask\n",
      "meat\n",
      "poison\n",
      "ghost \n",
      "spider\n",
      "potio"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(10000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
